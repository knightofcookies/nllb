{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Fine-tuning Meta's NLLB Distilled 600M](https://cointegrated.medium.com/how-to-fine-tune-a-nllb-200-model-for-translating-a-new-language-a37fc706b865)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "trans_df = pd.read_csv(\"parallel_corpus.tsv\", sep=\"\\t\\t\\t\\t\\t\", engine=\"python\")\n",
    "print(trans_df.shape)\n",
    "print(trans_df.columns)\n",
    "trans_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = trans_df[0:4950000].copy() # 4950000 items\n",
    "df_dev = trans_df[4950000:4975000].copy()     # 25000 items\n",
    "df_test = trans_df[4975000:].copy()   # 25000 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.src_lang = \"vie_Latn\"\n",
    "inputs = tokenizer(text=\"Mặt trời mọc ở phía đông\", return_tensors=\"pt\")\n",
    "translated_tokens = model.generate(\n",
    "    **inputs, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"eng_Latn\")\n",
    ")\n",
    "print(tokenizer.decode(translated_tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def word_tokenize(text):\n",
    "    \"\"\"\n",
    "    Split a text into words, numbers, and punctuation marks\n",
    "    (for languages where words are separated by spaces)\n",
    "    \"\"\"\n",
    "    return re.findall('(\\w+|[^\\w\\s])', text)\n",
    "\n",
    "smpl = df_train.sample(10000, random_state=1)\n",
    "smpl['kha_toks'] = smpl.kha.apply(tokenizer.tokenize)\n",
    "smpl['kha_words'] = smpl.kha.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = smpl[\n",
    "    ['kha_toks', 'kha_words']\n",
    "].applymap(len).describe()\n",
    "print(stats.kha_toks['mean'] / stats.kha_words['mean'])  # 1.3971\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm, trange\n",
    "import random\n",
    "\n",
    "texts_with_unk = [\n",
    "    text for text in tqdm(trans_df.kha) \n",
    "    if tokenizer.unk_token_id in tokenizer(text).input_ids\n",
    "]\n",
    "print(len(texts_with_unk))\n",
    "s = random.sample(texts_with_unk, 5)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "from sacremoses import MosesPunctNormalizer\n",
    "\n",
    "mpn = MosesPunctNormalizer(lang=\"en\")\n",
    "mpn.substitutions = [\n",
    "    (re.compile(r), sub) for r, sub in mpn.substitutions\n",
    "]\n",
    "\n",
    "def get_non_printing_char_replacer(replace_by: str = \" \"):\n",
    "    non_printable_map = {\n",
    "        ord(c): replace_by\n",
    "        for c in (chr(i) for i in range(sys.maxunicode + 1))\n",
    "        # same as \\p{C} in perl\n",
    "        # see https://www.unicode.org/reports/tr44/#General_Category_Values\n",
    "        if unicodedata.category(c) in {\"C\", \"Cc\", \"Cf\", \"Cs\", \"Co\", \"Cn\"}\n",
    "    }\n",
    "\n",
    "    def replace_non_printing_char(line) -> str:\n",
    "        return line.translate(non_printable_map)\n",
    "\n",
    "    return replace_non_printing_char\n",
    "\n",
    "replace_nonprint = get_non_printing_char_replacer(\" \")\n",
    "\n",
    "def preproc(text):\n",
    "    clean = mpn.normalize(text)\n",
    "    clean = replace_nonprint(clean)\n",
    "    # replace 𝓕𝔯𝔞𝔫𝔠𝔢𝔰𝔠𝔞 by Francesca\n",
    "    clean = unicodedata.normalize(\"NFKC\", clean)\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_with_unk_normed = [\n",
    "    text for text in tqdm(texts_with_unk) \n",
    "    if tokenizer.unk_token_id in tokenizer(preproc(text)).input_ids\n",
    "]\n",
    "print(len(texts_with_unk_normed))  # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.optimization import Adafactor\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "model.cuda();\n",
    "optimizer = Adafactor(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    scale_parameter=False,\n",
    "    relative_step=False,\n",
    "    lr=1e-4,\n",
    "    clip_threshold=1.0,\n",
    "    weight_decay=1e-3,\n",
    ")\n",
    "scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "LANGS = [('en', 'eng_Latn'), ('kha', 'vie_Latn')]\n",
    "\n",
    "def get_batch_pairs(batch_size, data=df_train):\n",
    "    (l1, long1), (l2, long2) = random.sample(LANGS, 2)\n",
    "    xx, yy = [], []\n",
    "    for _ in range(batch_size):\n",
    "        item = data.iloc[random.randint(0, len(data)-1)]\n",
    "        xx.append(preproc(item[l1]))\n",
    "        yy.append(preproc(item[l2]))\n",
    "    return xx, yy, long1, long2\n",
    "\n",
    "print(get_batch_pairs(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Try to free GPU memory\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_length = 128  # token sequences will be truncated\n",
    "training_steps = 600000  # Usually, I set a large number of steps,\n",
    "# and then just interrupt the training manually\n",
    "losses = []  # with this list, I do very simple tracking of average loss\n",
    "MODEL_SAVE_PATH = 'saved_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model.train()\n",
    "x, y, loss = None, None, None\n",
    "cleanup()\n",
    "\n",
    "tq = trange(len(losses), training_steps)\n",
    "for i in tq:\n",
    "    xx, yy, lang1, lang2 = get_batch_pairs(batch_size)\n",
    "    try:\n",
    "        tokenizer.src_lang = lang1\n",
    "        x = tokenizer(xx, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n",
    "        tokenizer.src_lang = lang2\n",
    "        y = tokenizer(yy, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n",
    "        # -100 is a magic value ignored in the loss function\n",
    "        # because we don't want the model to learn to predict padding ids\n",
    "        y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        loss = model(**x, labels=y.input_ids).loss\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "    except RuntimeError as e:  # usually, it is out-of-memory\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        x, y, loss = None, None, None\n",
    "        cleanup()\n",
    "        print('error', max(len(s) for s in xx + yy), e)\n",
    "        continue\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        # each 1000 steps, I report average loss at these steps\n",
    "        print(i, np.mean(losses[-1000:]))\n",
    "\n",
    "    if i % 1000 == 0 and i > 0:\n",
    "        model.save_pretrained(MODEL_SAVE_PATH)\n",
    "        tokenizer.save_pretrained(MODEL_SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
